{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import chrom drivers\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pygsheets\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Craigslist Ad Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Scraper Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses html\n",
    "def get_updated_html():\n",
    "    thesoup = BeautifulSoup(browser.html, 'html.parser')\n",
    "\n",
    "    return thesoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data from ad and populate the google sheet\n",
    "def scrape_ad(row_count,the_html,current_link):\n",
    "    #print(row_count)\n",
    "    can_reply=True\n",
    "    #get html and pull data from the page\n",
    "    #the_html=get_updated_html()\n",
    "\n",
    "\n",
    "    #job type\n",
    "    job_title=\"\"\n",
    "    the_cats=the_html.find_all(\"span\")\n",
    "    for cat in the_cats:\n",
    "        if str(cat).__contains__(\"job title\"):\n",
    "            job_title=str(cat).split('<b>')[1].split('</b>')[0]\n",
    "            #print(job_title)\n",
    "\n",
    "    #compensation\n",
    "    compensation=\"\"\n",
    "    the_cats=the_html.find_all(\"span\")\n",
    "    for cat in the_cats:\n",
    "        if str(cat).__contains__(\"compensation\"):\n",
    "            compensation=(str(cat).split('<b>')[1].split('</b>')[0])\n",
    "            #print(compensation)\n",
    "\n",
    "    #employment type\n",
    "    employment_type=\"\"\n",
    "    the_cats=the_html.find_all(\"span\")\n",
    "    for cat in the_cats:\n",
    "        if str(cat).__contains__(\"employment type\"):\n",
    "            employment_type=(str(cat).split('<b>')[1].split('</b>')[0])\n",
    "            #print(employment_type)\n",
    "\n",
    "    #company name\n",
    "    if(len(the_html.find_all('h2'))!=0):\n",
    "        company_name=str(the_html.find_all('h2')[0]).split('>')[1].split('<')[0]\n",
    "    else:\n",
    "        company_name=input('Enter company name manually (or leave empty to skip): ')\n",
    "    if company_name=='':\n",
    "        can_reply=False\n",
    "        company_name='skip'\n",
    "\n",
    "    #title\n",
    "    title=str(the_html.find_all('span',id=\"titletextonly\")[0]).split('>')[1].split('<')[0]\n",
    "    #print(title)\n",
    "\n",
    "    #posting body\n",
    "    posting_body=str(the_html.find_all('section',id=\"postingbody\")).split('</div>\\n')[2].split('</section')[0]\n",
    "    #print(posting_body)\n",
    "\n",
    "    #date posted\n",
    "    posted=\"\"\n",
    "    the_cats=the_html.find_all(\"p\")\n",
    "\n",
    "    for cat in the_cats:\n",
    "        if str(cat).__contains__('date timeago\"') & str(cat).__contains__('Posted'):\n",
    "            posted=(str(cat).split('datetime=\"')[1].split('\"')[0])\n",
    "            #print(posted)\n",
    "\n",
    "    #post id\n",
    "    post_id=\"\"\n",
    "    the_cats=the_html.find_all(\"p\")\n",
    "    #print(the_cats)\n",
    "    for cat in the_cats:\n",
    "        if str(cat).__contains__('post id'):\n",
    "            post_id=(str(cat).split('post id: ')[1].split('</p>')[0])\n",
    "            #print(post_id)\n",
    "    \n",
    "    #city location and area\n",
    "    city=the_html.find_all('a')[1].text\n",
    "    city_area=the_html.find_all('a')[2].text        \n",
    "            \n",
    "    #post email\n",
    "    contbutt=\"\"\n",
    "    if company_name!='skip' and (post_id not in stored_ids):\n",
    "        if company_name+city+city_area not in stored_restaurants and str(job_title).__contains__('washer')==False \\\n",
    "        and str(job_title).__contains__('cleaner')==False and str(job_title).__contains__('Cleaner')==False:\n",
    "            if len(browser.find_by_tag('button'))!=0:\n",
    "                browser.find_by_tag('button')[0].click()\n",
    "                time.sleep(.5)\n",
    "                #temp_button=browser.find_by_text('show address')\n",
    "                temp_button=browser.find_by_text('email')\n",
    "                if (len(temp_button)==0):\n",
    "                    temp_button=browser.find_by_text('show address')\n",
    "                    if (len(temp_button)==0):\n",
    "                        contbutt=input('<Return> to Continue when Captcha Code is entered (or <x> to skip ad)') #if needed to verify  \n",
    "                \n",
    "                if contbutt==\"\":\n",
    "                    temp_button=browser.find_by_text('email')\n",
    "                    if (len(temp_button)!=0):\n",
    "                        browser.find_by_text('email').click()\n",
    "                    \n",
    "                    temp_button=browser.find_by_text('show address')\n",
    "                    if (len(temp_button)!=0):\n",
    "                        browser.find_by_text('show address').click()     \n",
    "                    \n",
    "                    time.sleep(.5)\n",
    "                    the_html=get_updated_html()\n",
    "                    email_link=\"\"\n",
    "                    the_links=the_html.find_all('a')\n",
    "\n",
    "                    for link in the_links:\n",
    "                        if str(link).__contains__('mailto'):\n",
    "                            email_link=(str(link).split('mailto:')[1].split('?')[0])\n",
    "\n",
    "                    browser.find_by_tag('button')[0].click() \n",
    "                else:\n",
    "                    can_reply=False\n",
    "                    email_link=\"\"\n",
    "                \n",
    "            else:\n",
    "                email_link=input('Manually enter email or leave blank to skip')\n",
    "        else:\n",
    "            can_reply=False\n",
    "            email_link=\"\"\n",
    "    else:\n",
    "        email_link=\"\"      \n",
    "    \n",
    "    if email_link!=\"\" and can_reply==True and (post_id not in stored_ids):\n",
    "        print('success')\n",
    "        wks.update_value(f'A{row_count}', company_name)\n",
    "        wks.update_value(f'B{row_count}', job_title)\n",
    "        wks.update_value(f'C{row_count}', compensation)\n",
    "        wks.update_value(f'D{row_count}', employment_type)\n",
    "        wks.update_value(f'E{row_count}', title)\n",
    "        wks.update_value(f'F{row_count}', posted)\n",
    "        wks.update_value(f'G{row_count}', post_id)\n",
    "\n",
    "        wks.update_value(f'H{row_count}', email_link)\n",
    "        wks.update_value(f'I{row_count}', city)\n",
    "        wks.update_value(f'J{row_count}', city_area)\n",
    "        wks.update_value(f'K{row_count}', \"No\")\n",
    "        wks.update_value(f'L{row_count}', \"None\")\n",
    "        wks.update_value(f'O{row_count}', current_link)\n",
    "        \n",
    "        stored_restaurants.append(company_name+city+city_area)\n",
    "        return True\n",
    "    else: \n",
    "        print('unsuccessful') #if it wasn't able to scrape ad it will return this\n",
    "        return False\n",
    "        time.sleep(.2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraper to push the 'next' button\n",
    "def click_next():\n",
    "    next_buttons=browser.find_by_tag('button')\n",
    "    for button in next_buttons:\n",
    "        if str(button['class']).__contains__('bd-button cl-next-page'):\n",
    "            temp_button=button\n",
    "            \n",
    "    temp_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before scraping, thiscd hecks the csv list of already scraped ads to ensure it has not been visited \n",
    "def get_sheet_values_for_comparison():\n",
    "    a=wks.get_col(1)\n",
    "    b=wks.get_col(9)\n",
    "    c=wks.get_col(10)\n",
    "\n",
    "    d=[]\n",
    "    for count, aa in enumerate(a):\n",
    "        if aa !=\"\":\n",
    "            d.append(a[count]+b[count]+c[count])\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls ad ids for further comparison to ensure no ad is scraped twice\n",
    "def get_stored_ids():\n",
    "    a=wks.get_col(7)\n",
    "\n",
    "    d=[]\n",
    "    for count, aa in enumerate(a):\n",
    "        if aa !=\"\":\n",
    "            d.append(a[count])\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drives the webscraper\n",
    "def scrap_cragsl(scroll_pages_deep,avoids,the_city):\n",
    "    input('Make sure you are on the top food/beverage/hospitality page (press return when you are ready)')\n",
    "    \n",
    "    row_count=len(wks.get_values_batch('A')[0])+1 #finds first empty row\n",
    "    df=pd.read_csv(f'visted_ads_{the_city}.csv')\n",
    "\n",
    "    link_list=[] #stores links to search\n",
    "\n",
    "\n",
    "    #scroll through pages and grab links\n",
    "    for x in range(scroll_pages_deep):\n",
    "        #grab ad links           \n",
    "        page_links=browser.find_by_tag('a')                   \n",
    "        for link in page_links:\n",
    "            if(link.has_class(\"titlestring\")==True):\n",
    "                in_avoid_list=False\n",
    "                for avoid in avoids:\n",
    "                    if (str(link['href']).upper().__contains__(avoid.upper())==True):\n",
    "                        in_avoid_list=True              \n",
    "                if (link['href'] not in df['links'].tolist() and in_avoid_list==False):\n",
    "                    link_list.append(link['href'])\n",
    "\n",
    "    limit_search_to_ads=input(f'Found {len(link_list)} new ads. How many do you want to scrape? ')\n",
    "\n",
    "    #scan ads\n",
    "    for x in range(len(link_list)):\n",
    "        print(x)\n",
    "        if x==int(limit_search_to_ads):\n",
    "            break\n",
    "\n",
    "        #visit ad    \n",
    "        if(link_list[x] not in df['links'].tolist()):\n",
    "            skip_ad=input(f'Scrape: {link_list[x]}? (leave BLANK to scrape ad)')\n",
    "            if(skip_ad==''):\n",
    "                the_url=link_list[x]\n",
    "                browser.visit(the_url)\n",
    "                time.sleep(1)\n",
    "                temp_html=get_updated_html()\n",
    "                check_if_added=scrape_ad(row_count,temp_html,link_list[x])\n",
    "                #append df\n",
    "                df.loc[len(df.index)]=link_list[x]\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                print('skipped ad')\n",
    "                time.sleep(.2)\n",
    "                check_if_added=False\n",
    "                df.loc[len(df.index)]=link_list[x]\n",
    "                #write back to csv to avoid visiting pages again\n",
    "            df.to_csv(f'visted_ads_{the_city}.csv',index=False)\n",
    "            if check_if_added==True:\n",
    "                row_count=row_count+1\n",
    "            clear_output(wait=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Chrome Driver and Run Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the city to scrape \n",
    "city_csv='miami'\n",
    "\n",
    "\n",
    "#Sets up engine\n",
    "executable_path= {'executable_path': ChromeDriverManager().install()}\n",
    "browser= Browser('chrome', **executable_path, headless=False)\n",
    "time.sleep(3)\n",
    "browser.visit(f'https://{city_csv}.craigslist.org/search/fbh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Google Sheets\n",
    "client = pygsheets.authorize(service_file='cred4.json')\n",
    "sh=client.open('craigslist_data')\n",
    "wks = sh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 ads scraped\n",
      "Google Sheet updated\n"
     ]
    }
   ],
   "source": [
    "# How many pages deep do you want to scroll back (usually 1 page is enough (about 100 ads))\n",
    "pages_deep=1\n",
    "\n",
    "#a list of job titles to avoid\n",
    "titles_to_avoid=['doordash','instacart','domino','fishing','shopper','travel','dishwasher','McMenamins','smyrna',\\\n",
    "                'superica','manager']\n",
    "\n",
    "\n",
    "#creates a list of already stored restaurants\n",
    "stored_restaurants=get_sheet_values_for_comparison()\n",
    "stored_ids=get_stored_ids()\n",
    "\n",
    "\n",
    "scrap_cragsl(pages_deep,titles_to_avoid,city_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Emails\n",
    "\n",
    "This section pulls the stored emails from the Google Sheet, composes a unique cover letter using data from the ad, then attaches the appropriate resume for the position offered, before sending the email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### email functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import smtplib\n",
    "from email import encoders\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to set up app password. See link below if you do not have an app password:\n",
    "# https://help.warmupinbox.com/en/articles/4934806-configure-for-google-workplace-with-two-factor-authentication-2fa\n",
    "\n",
    "def send_email(resume_ver,subject,ap_name,num,sender_email,new_title,sender_password):\n",
    "\n",
    "    \n",
    "    recipient_email = wks.get_value(f'H{num}')\n",
    "    #recipient_email = 'julia.claira@berkeley.edu'\n",
    "    subject = f\"Re: Craigslist {wks.get_value(f'E{num}')}\"\n",
    "    body = compose_email(ap_name,num,new_title)\n",
    "\n",
    "\n",
    "    with open(pull_resume(resume_ver,wks.get_value(f'I{num}')), \"rb\") as attachment:\n",
    "        # Add the attachment to the message\n",
    "        part = MIMEBase(\"application\", \"octet-stream\")\n",
    "        part.set_payload((attachment).read())\n",
    "    encoders.encode_base64(part)\n",
    "    part.add_header(\n",
    "        \"Content-Disposition\",\n",
    "        f\"attachment; filename= {os.path.basename(pull_resume(resume_ver,wks.get_value(f'I{num}')))}\",\n",
    "    )\n",
    "\n",
    "    message = MIMEMultipart()\n",
    "    message['Subject'] = subject\n",
    "    message['From'] = sender_email\n",
    "    message['To'] = recipient_email\n",
    "    html_part = MIMEText(body)\n",
    "    message.attach(html_part)\n",
    "    message.attach(part)\n",
    "\n",
    "    server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "    server.login(sender_email, sender_password)\n",
    "    server.sendmail(sender_email, recipient_email, message.as_string())\n",
    "    server.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the body of the email.\n",
    "def compose_email(applicant,num,new_title):\n",
    "    \n",
    "    the_position=new_title\n",
    "    the_company=wks.get_value(f'A{num}')\n",
    "\n",
    "\n",
    "    if the_company!=\"\":\n",
    "        emailbody= f'\\\n",
    "Hi,\\n\\n\\\n",
    "I am interested in the {the_position} position that you posted on Craigslist. \\\n",
    "I have always been passionate about food and customer service, and I believe that I would make a great addition to your team at {the_company}. \\\n",
    "My resume is attached. \\n \\n\\\n",
    "Thank you, \\n \\n\\\n",
    "{applicant}'\n",
    "        \n",
    "    else:\n",
    "        emailbody= f'\\\n",
    "Hi,\\n\\n\\\n",
    "I am interested in the {the_position} position that you posted on Craigslist. \\\n",
    "I have always been passionate about food and customer service, and I believe that I would make a great addition to your team. \\\n",
    "My resume is attached. \\n \\n\\\n",
    "Thank you, \\n \\n\\\n",
    "{applicant}'\n",
    "\n",
    "\n",
    "    return emailbody\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the resume to attach based on city name and job position\n",
    "\n",
    "def pull_resume(x,city):\n",
    "    match x:\n",
    "        case 1:\n",
    "            return f\"241_resumes/{city}/server/Charles_Miller_Resume.pdf\"\n",
    "        case 2:\n",
    "            return f\"241_resumes/{city}/server/Claire_Miller_Resume.pdf\"\n",
    "        case 3:\n",
    "            return f\"241_resumes/{city}/cook/Charles_Miller_Resume.pdf\"\n",
    "        case 4:\n",
    "            return f\"241_resumes/{city}/cook/Claire_Miller_Resume.pdf\"\n",
    "        case 5:\n",
    "            return f\"241_resumes/{city}/server/Cypress_Miller_Resume.pdf\"\n",
    "        case 6:\n",
    "            return f\"241_resumes/{city}/cook/Cypress_Miller_Resume.pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sends the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Google Sheets\n",
    "client = pygsheets.authorize(service_file='cred4.json')\n",
    "sh=client.open('craigslist_data')\n",
    "wks = sh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send emails between these rows \n",
    "range_begin=691\n",
    "range_end=733"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 emails sent!\n",
      "Google Sheet updated\n"
     ]
    }
   ],
   "source": [
    "# After Sending emails this will update the Google Sheet\n",
    "\n",
    "#Import passwords for emails\n",
    "from craigslist_p import psw\n",
    "\n",
    "#pull columns\n",
    "email_col=wks.get_col(8)\n",
    "resume_sent_col=wks.get_col(11)\n",
    "title_col=wks.get_col(5)\n",
    "length_of_sh=len(wks.get_values_batch('A')[0])+1\n",
    "\n",
    "email_version=1\n",
    "version=1 #1 for Charles and 2 for Claire 3 for Cypress\n",
    "applicant_name=[\"\",\"Charles\",\"Claire\",\"Charles\",\"Claire\",\"Cypress\",\"Cypress\"]\n",
    "\n",
    "city_opt1 ={\"new york\":[\"\",\"c.miller.37949\",\"c.miller.51355\",\"c.miller.62235\"],\\\n",
    "               \"south florida\":[\"\",\"c.miller.25804\",\"c.miller.78882\",\"c.miller.81313\"],\\\n",
    "               \"chicago\":[\"\",\"c.miller.14523\",\"c.miller.40104\",\"c.miller.14523\"]}\n",
    "\n",
    "city_opt2={\"new york\":[\"\",\"c.miller.37949\",\"c.miller.51355\",\"c.miller.62235\"],\\\n",
    "                       \"south florida\":[\"\",\"c.miller.25804\",\"c.miller.78882\",\"c.miller.81313\"],\\\n",
    "                       \"chicago\":[\"\",\"c.miller.40104\",\"c.miller.14523\",\"c.miller.40104\"]}\n",
    "\n",
    "sender_email_d = city_opt1\n",
    "\n",
    "the_pswrds={'c.miller.37949@gmail.com':psw[1],\"c.miller.51355@gmail.com\":psw[2],\\\n",
    "                \"c.miller.25804@gmail.com\":psw[3],\\\n",
    "                 \"c.miller.78882@gmail.com\":psw[4],'c.miller.14523@gmail.com':psw[5],\\\n",
    "            \"c.miller.40104@gmail.com\":psw[6],\"c.miller.62235@gmail.com\":psw[7],\\\n",
    "           \"c.miller.81313@gmail.com\":psw[8],\"c.miller.99811@gmail.com\":psw[9]} \n",
    "\n",
    "#send emails\n",
    "found_cities=True\n",
    "city_count=1\n",
    "city_e='new york'\n",
    "city_pop=['new york','south florida','chicago']\n",
    "sf=1\n",
    "\n",
    "while found_cities==True:\n",
    "    \n",
    "    found_cities=False\n",
    "    for i in range(range_begin,range_end):\n",
    "\n",
    "        if city_count <=3:\n",
    "            city_e='new york'                  \n",
    "        elif city_count<=6 and city_count>3:\n",
    "            city_e='south florida'                       \n",
    "        else:            \n",
    "            city_e='chicago'           \n",
    "        \n",
    "        if(resume_sent_col[i-1]=='No' and wks.get_value(f'I{i}')==city_e):\n",
    "            found_cities=True\n",
    "\n",
    "            #update resume sent column\n",
    "            wks.update_value(f'K{i}', 'Yes')\n",
    "            #add time resume sent\n",
    "            wks.update_value(f'M{i}', str(datetime.now()))\n",
    "\n",
    "            recorded_title=wks.get_value(f'B{i}')\n",
    "\n",
    "            the_title=input(f'Title: {recorded_title} - Leave BLANK to keep as is, or type new title: ')\n",
    "            which_resume=input('Server or Cook resume (enter \"s\" or \"c\")')\n",
    "            if (which_resume !='s'):\n",
    "                which_resume='c'\n",
    "                if version!=5:\n",
    "                    version=version+2\n",
    "                else:\n",
    "                    version=version+1\n",
    "\n",
    "            #add resume version sent\n",
    "            wks.update_value(f'N{i}', version)\n",
    "\n",
    "            if the_title=='':\n",
    "                    the_title=wks.get_value(f'B{i}')\n",
    "\n",
    "            sender_version=sender_email_d[wks.get_value(f'I{i}')][email_version]+'@gmail.com'\n",
    "\n",
    "            #send email\n",
    "            send_email(version,title_col[i],applicant_name[version],i,sender_version,the_title,\\\n",
    "                      the_pswrds[sender_version])\n",
    "            time.sleep(.2)\n",
    "\n",
    "            wks.update_value(f'P{i}', sender_version)\n",
    "            wks.update_value(f'Q{i}', which_resume)\n",
    "\n",
    "            print(city_e)\n",
    "            print(applicant_name[version])\n",
    "            print(f'resume version: {version}')\n",
    "            print(f'row {i}')\n",
    "            time.sleep(3)\n",
    "\n",
    "            #update email version\n",
    "            if email_version !=3:\n",
    "                email_version=email_version+1\n",
    "            else:\n",
    "                email_version =1\n",
    "\n",
    "            if version==2 or version==4:\n",
    "                    version=5\n",
    "            elif version==1 or version==3:\n",
    "                    version=2\n",
    "            else:\n",
    "                version=1\n",
    "\n",
    "            clear_output(wait=False)\n",
    "            city_count=city_count+1\n",
    "            if city_count>=10:\n",
    "                city_count=1\n",
    "                if sf==1:\n",
    "                    sender_email_d=city_opt2\n",
    "                    sf=2\n",
    "                    \n",
    "                else:\n",
    "                    sf=1\n",
    "                    sender_email_d=city_opt1\n",
    "                    \n",
    "    if(city_e not in city_pop and found_cities==False):\n",
    "        city_count=city_count+3\n",
    "                \n",
    "    elif (found_cities==False and city_e in city_pop):\n",
    "        city_pop.remove(city_e)\n",
    "        print('city_pop')\n",
    "        city_count=city_count+3\n",
    "        \n",
    "    if len(city_pop)==0:\n",
    "        found_cities=False\n",
    "    else:\n",
    "        found_cities=True\n",
    "    if city_count>=10:\n",
    "        city_count=1\n",
    "        if sf==1:\n",
    "            sender_email_d=city_opt2\n",
    "            sf=2\n",
    "            print('shift sf')\n",
    "        else:\n",
    "            sf=1\n",
    "            sender_email_d=city_opt1\n",
    "    \n",
    "    print(city_count)\n",
    "    print(f'sf: {sf}')\n",
    "    time.sleep(1)\n",
    "    resume_sent_col=wks.get_col(11)\n",
    "    time.sleep(1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claira",
   "language": "python",
   "name": "claira"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
